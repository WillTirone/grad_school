---
title: "STA 602 HW 12"
author: William Tirone
format: pdf
---

```{r echo=FALSE}
suppressPackageStartupMessages({
library(MASS) 
library(tidyverse)
library(caret)
})
```

# 9.1

```{r}
# load data 
swim = as.matrix(read.table('http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat'))
```

## a)

Referencing Hoff p. 154/155:

```{r}

# data 
# response is weeks 
X = cbind(rep(1, 6), seq(1, 11, 2))
n = dim(X)[1]

# priors 
B0 = c(23,0)
Sig0 = matrix(c(0.25,0,0,0.1),2,2,byrow = TRUE)
nu0 = 1 
sig2.0 = 0.25

# iterations 
S = 5000 

# initial values 
B = B0 
sig2 = sig2.0

#output 
beta_out = matrix(NA,nrow = S,ncol = 2)
sig2_out = c()

for (j in 1:4){
  
  # data for jth swimmer
  y = swim[j,]
  
  # gibb's sample linear model 
  for (i in 1:S) {
    
    B.curr = B 
    sig2.curr = sig2
    
    # updating Beta 
    V = solve(solve(Sig0)  +  (t(X) %*% X) / sig2.curr) 
    m = V %*% (solve(Sig0) %*% B0 + (t(X) %*% y)/sig2.curr )
    B = mvrnorm(1,m,V)
    
    # updating sigma2 
    SSR = t(y - X %*% B) %*% (y - X %*% B)
    sig2 = 1 / rgamma(1, (nu0 + n)/2, (nu0 * sig2.0 + SSR)/2)
    
    # store values
    beta_out[i,] = B
    sig2_out = append(sig2_out, sig2)
    
  }
  
  x_predict = c(1,13)
  y_predict = rnorm(S, beta_out %*% x_predict, sqrt(sig2_out))
  
  print(mean(y_predict))
}

```

## b)

Since we want the fastest swimmer, using the predictive distribution (this is just a normal model since we have a MVN sampling and inverse gamma prior) we see that the first swimmer has the fastest mean time at 22.63978. So we will choose swimmer 1 to race.

# 9.2

```{r}
az = as.matrix(read.table('http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/azdiabetes.dat', header=T))
```

## a)

Hoff p. 157

We can sample from $p(\sigma^2 | y, X)$ and $p(\beta | y,X,\sigma^2)$ directly and can use vanilla MC to do so.

```{r}

# make subset numeric 
dat = apply(az[,-8], 2, as.numeric)

# data 
X = dat[,-2]
y = dat[,2] # response, glucose 
n = dim(X)[1]
p = dim(X)[2]
I = diag(1,n,n)

# priors 
g = n
nu0 = 2 
sig2.0 = 1

# samples 
S = 5000

# sample sigma^2 
SSRg = t(y) %*% (I - (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)) %*% y
sig2 = 1 / rgamma(S, (nu0 + n)/2, (nu0 * sig2.0 + SSRg)/2)

# sample Beta 
Vb = (g/(g+1)) * solve(t(X) %*% X)
Eb = Vb %*% t(X) %*% y
E = matrix(rnorm(S*p,0,sqrt(sig2)),S,p)
beta = t(t(E %*% chol(Vb)) + c(Eb))
```

Confidence Regions for the variables are below:

```{r}
cat("Sigma^2 95% Confidence Region: ",quantile(sig2,c(0.025,0.975)))

custom_quant = function(X) {
  quantile(X, c(0.025,0.975), na.rm = T)
}

apply(beta,2,custom_quant)
```

Comparing the MC results to a standard `lm()` fit out of curiosity. The intervals capture most of the betas below fairly well, though some do not if they have large standard errors.

```{r}
test_linear = lm(glu ~ ., data.frame(dat))
summary(test_linear)
```

## b)

```{r cache=TRUE}


# calculate log(p(y|X))
lpy.X <- function(y, X, g=length(y), nu0=1,
                  s20=try(summary(lm(y ~ 1+X))$sigma^2,
                          silent=TRUE))
  {
    n <- dim(X)[1];p <- dim(X)[2]
    if(p==0){Hg <- 0;s20 <- mean(y^2)}
      if(p>0){Hg <- (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)}
        SSRg <- t(y) %*% (diag(1,nrow=n)-Hg) %*% y 
        -.5*(n*log(pi) + p*log(1 + g) + (nu0 + n)*log(nu0*s20 + SSRg)-
        nu0*log(nu0*s20)) + 
        lgamma((nu0 + n)/2)-lgamma(nu0/2)
  }

# set up 
z <- rep(1,dim(X)[2])
lpy.c <- lpy.X(y,X[,z==1,drop=FALSE])
S <- 10000
Z <- matrix(NA,S,dim(X)[2])

beta_output = data.frame()


# Gibb's sampler 
for(s in 1:S)
{
  for(j in sample(1:dim(X)[2]))
    {
      zp <- z;zp[j] <- 1-zp[j]
      lpy.p <- lpy.X(y,X[,zp==1,drop=FALSE])
      r <- (lpy.p-lpy.c)*(-1)^(zp[j]==0)
      z[j] <- rbinom(1,1,1/(1 + exp(-r)))
      if(z[j]==zp[j]){lpy.c <- lpy.p}
    }
  
  # create an X_z for Z = 1 
  X_z = X[,c(which(z==1))]
  
  # sample sigma^2 
  SSRg_z = t(y) %*% (I - (g/(g+1)) * 
                       X_z %*% solve(t(X_z) %*% X_z) %*% t(X_z)) %*% y
  sig2 = 1 / rgamma(1, (nu0 + n)/2, (nu0 * sig2.0 + SSRg_z)/2)
  
  # sample Beta 
  Vb = g * sig2 * solve(t(X_z) %*% X_z)
  m = matrix(0,dim(Vb)[1],1)
  beta = mvrnorm(1,m,Vb)
  
  # save results 
  Z[s,] <- z
  
  beta_output = dplyr::bind_rows(beta_output,beta)
}
```

Below, finding $p(\beta_j \ne 0 | y)$ and confidence intervals

I'm not entirely sure I did this correctly but my reasoning is that if the model selected NA for that particular beta, then the posterior probability would just be calculated in the usual Monte Carlo way of calculating the average number of values that are not zero. There are no NAs for the "bp" column, for example, so the probability that it is not zero is equal to 1, so we should always use that column.

```{r}
# number of NA values 
1 - (apply(apply(beta_output, 2, is.na),2,sum) / 1000)
```

Confidence Intervals: These look quite different compared to part a. The variables that look the most different have a very low probability of being selected (computed above); for example, skin has a huge interval but a very small one in part a, the same with npreg. I think this expresses the model averaging suggesting these shouldn't be used.

```{r}
apply(beta_output,2,custom_quant)
```

# 9.3

```{r echo=FALSE}
crime = as.matrix(
  read.table('http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/crime.dat',
             header = T))
```

## a)

Adapting code from 9.2 a), we can use vanilla MC.

It looks like M, Ed, U2, Ineq, and Prob (we can view the definitions of these with ?UScrime if desired) reach statistical significance with a linear regression with lm. The relationships for all of these are positive, except for Prob which has a negative relationship.

From a quick visual inspection, the marginal posterior means are very close to the OLS estimates. The posterior confidence intervals do not have an equivalent frequentist interpretation, but allow us to make probabilistic statements about the coefficients.

```{r}

# make subset numeric 
dat = apply(crime, 2, as.numeric)

# data 
X = dat[,-1]
y = dat[,1] # response 
n = dim(X)[1]
p = dim(X)[2]
I = diag(1,n,n)

# priors 
g = n
nu0 = 2 
sig2.0 = 1

# samples 
S = 5000

# sample sigma^2 
SSRg = t(y) %*% (I - (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)) %*% y
sig2 = 1 / rgamma(S, (nu0 + n)/2, (nu0 * sig2.0 + SSRg)/2)

# sample Beta 
Vb = (g/(g+1)) * solve(t(X) %*% X)
Eb = Vb %*% t(X) %*% y
E = matrix(rnorm(S*p,0,sqrt(sig2)),S,p)
beta = t(t(E %*% chol(Vb)) + c(Eb))
```

Obtaining marginal posterior means and 95% CI:

```{r}
print("Posterior Beta Means")
as.matrix(apply(beta,2,mean))
```

```{r}
print("95% CI")
apply(beta,2,custom_quant)
```

Now fitting OLS:

```{r}
OLS_fit = lm(y ~ ., data.frame(crime))
summary(OLS_fit)
```

## b)

Looking at the next several cells, the lm fit achieves an MSE of .61 compared to .47 for the Bayesian approach with a g prior. I would guess the Bayesian model does a better job because it more accurately captures whether or not a covariate should be used by using probability values rather than the frequentist approach.

i\)

Predicted values are plotted compared to $y_{te}$ below.

```{r}
# reload to be safe 
crime = read.table('http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/crime.dat',
             header = T)

y = crime[,'y']
X = crime[,-1] # not y 

index = createDataPartition(y,1)


train = crime[index$Resample1,]
test = crime[-index$Resample1,]

train_fit = lm(y ~ ., data = train)
pred_y = predict(train_fit, newdata = test)

plot(pred_y, test$y)
```

Looking at coefficients for lm:

```{r}
summary(train_fit)$coefficients
```

Now computing MSE:

```{r}
mean((test$y - pred_y)^2)
```

ii\) Now with the g prior:

```{r}

# make subset numeric 
dat = apply(train, 2, as.numeric)

# data 
X = dat[,-1]
y = dat[,1] # response 
n = dim(X)[1]
p = dim(X)[2]
I = diag(1,n,n)

# priors 
g = n
nu0 = 2 
sig2.0 = 1

# samples 
S = 5000

# sample sigma^2 
SSRg = t(y) %*% (I - (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)) %*% y
sig2 = 1 / rgamma(S, (nu0 + n)/2, (nu0 * sig2.0 + SSRg)/2)

# sample Beta 
Vb = (g/(g+1)) * solve(t(X) %*% X)
Eb = Vb %*% t(X) %*% y
E = matrix(rnorm(S*p,0,sqrt(sig2)),S,p)
beta = t(t(E %*% chol(Vb)) + c(Eb))

print("posterior means : ")
beta_hat_bayes = as.matrix(apply(beta,2,mean))
beta_hat_bayes
```

Now obtaining the predictions $\hat{y}$

```{r}
test_bayes = apply(test[,-1], 2, as.numeric)
y_hat_bayes = test_bayes %*% beta_hat_bayes # predictions for data 

plot(y_hat_bayes, test$y)
```

Prediction (Mean Squared) error:

```{r}
mean((test$y - y_hat_bayes)^2)
```

## c)

With many repetitions, the results look very similar and both methods produce extremely similar results and the fit between the two is almost linear.

```{r}
# helper function for g prior 
# should have done this earlier
bayes_regression = function(data){
  
  # make subset numeric 
  dat = apply(data, 2, as.numeric)
  
  # data 
  X = dat[,-1]
  y = dat[,1] # response 
  n = dim(X)[1]
  p = dim(X)[2]
  I = diag(1,n,n)
  
  # priors 
  g = n
  nu0 = 2 
  sig2.0 = 1
  
  # samples 
  S = 5000
  
  # sample sigma^2 
  SSRg = t(y) %*% (I - (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)) %*% y
  sig2 = 1 / rgamma(S, (nu0 + n)/2, (nu0 * sig2.0 + SSRg)/2)
  
  # sample Beta 
  Vb = (g/(g+1)) * solve(t(X) %*% X)
  Eb = Vb %*% t(X) %*% y
  E = matrix(rnorm(S*p,0,sqrt(sig2)),S,p)
  beta = t(t(E %*% chol(Vb)) + c(Eb))
  
  #compute means 
  beta_m = as.matrix(apply(beta,2,mean))
  
  return(beta_m)
  
}

bayes_regression(train)
```

## c)

Computing this 50 times with randomly generated splits:

```{r}
# reload to be safe 
crime = read.table('http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/crime.dat',
             header = T)


# create train / test index 
index = createDataPartition(crime$y,50)

lm_mse_output = c()
bayes_mse = c()

for (i in seq_len(length(index))){
  
  # train test split 
  train = crime[index[[i]],]
  test = crime[-index[[i]],]
  
  # fit lm and calculate mse 
  compare_lm = lm(y ~ ., train)
  lm_y_hat = predict(compare_lm,test[,-1])
  mse_lm = mean((test$y - lm_y_hat)^2)
  
  #output lm mse 
  lm_mse_output = c(lm_mse_output,mse_lm)
  
  # bayesian linear regression 
  bayes_y_hat = as.matrix(test[,-1]) %*% bayes_regression(train)
  bayes_mse_iter = mean((test$y - bayes_y_hat)^2)
  bayes_mse = c(bayes_mse, bayes_mse_iter)
}

comparison = data.frame(lm_mse = lm_mse_output,
           bayes_mse = bayes_mse)

plot(comparison$lm_mse,comparison$bayes_mse)
```

A view of the data for comparison:

```{r}
comparison
```
