{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22882a3-2af9-44ed-829d-d254864ed184",
   "metadata": {},
   "source": [
    "# STA 561 HW 4\n",
    "\n",
    "Authors  \n",
    " -  Alonso Guerrero Castaneda (UID: 1194613)  \n",
    " -  Eli Gnesin (UID: 1172961)  \n",
    " -  Tommy Misikoff (UID: 1166813)  \n",
    " -  Sanskriti Purohit (UID: 1179957)  \n",
    " -  Will Tirone (UID: 1130904)  \n",
    "\n",
    "TA: Rick Presman "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a9f19",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Consider data ${(T_i, \\delta_i)}_{i=1}^n$ as data and consider the Kaplan-Meier estimator $\\hat{S}_n(t) = \\prod_{j:t_j < t}(1 - \\frac{d_j}{n_j})$ where $n_j$ are the number of subjects still alive at time $t_j$ and $d_j$ are the number of subjects who died at time $t_j$ and $j$ is the index of observed event times. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fa4909a-0b9a-4944-88da-460c0a98a4e7",
   "metadata": {},
   "source": [
    "### Challenges Faced:\n",
    "\n",
    "In \"km_streaming.py\", it lacked accountability for risk values where delta = 0 and T_i > t_j appears before t_j in the dataset. One example can be taken from the first two elements of our observed data. They should account to risk values within the dataset but their risk values are never accounted for in the stream since we start storing for the values from our third element. \n",
    "    \n",
    "We attempted to resolve this issue by utilizing Reservoir Sampling to store a subset of our dataset to imitate and account for the behaviour of our data. This improves the estimation of our Kaplan-Meier estimator for most of our data-points with a few outliers as exceptions. This can be attributed to the fact that we are only looking at a subset of our actual data which may not account for the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7a4677-717f-4ccd-b770-3cf9712ae79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import tqdm\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "\n",
    "def streaming_km(\n",
    "        observed_times: np.ndarray,\n",
    "        censoring_ind: np.ndarray,\n",
    "        failure_set: Optional[dict] = {},\n",
    "        risk_set: Optional[dict] = {}\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute the Kaplan-Meier estimator and sufficient statistics with\n",
    "    single sweep through the data\n",
    "    :param observed_times: 1D np.array of observed times, i.e., the\n",
    "        minimum of failure and censoring times\n",
    "    :param censoring_ind: 1D np.array of indicators that an observation\n",
    "        was censored\n",
    "    :param failure_set: a dictionary mapping failure times to\n",
    "        the number of observed failures at that time, if None an empty\n",
    "        dictionary is created.  This allows us to sequentially process\n",
    "        multiple datasets.  Note that we do not changed passed dictionaries.\n",
    "    :param risk_set: a dictionary mapping failure times to the\n",
    "        number of individuals at risk.\n",
    "    :return: dictionary with key --> value pairs:\n",
    "        failure_set -> dictionary with keys corresponding to unique failure\n",
    "            times and values corresponding to number of observed failures at\n",
    "            that (key) time\n",
    "        risk_set -> dictionary with keys corresponding to unique failure\n",
    "            times and values corresponding to number of individuals at risk\n",
    "            at that (key) time\n",
    "        km -> dictionary with keys corresponding to unique failure times\n",
    "            and values corresponding to Kaplan-Meier estimator of\n",
    "            survivor function.\n",
    "    \"\"\"\n",
    "    \n",
    "    local_failure_set = failure_set.copy()\n",
    "    local_risk_set = risk_set.copy()\n",
    "    res_t = []\n",
    "    res_delta = []\n",
    "    K = 10\n",
    "    for x,(t, delta) in enumerate(zip(observed_times, censoring_indicator)):\n",
    "        if x < K:\n",
    "                res_t.append(t)\n",
    "                res_delta.append(delta)\n",
    "        else:\n",
    "            s = random.randint(0,x)\n",
    "            if s<K:\n",
    "                res_t[s] = t\n",
    "                res_delta[s] = delta\n",
    "        if delta == 1:\n",
    "            if t not in local_failure_set:\n",
    "                local_failure_set[t] = 0.0\n",
    "            local_failure_set[t] += 1.0\n",
    "            if t not in local_risk_set:\n",
    "                local_risk_set[t] = 0.0\n",
    "            unique_failure_times = np.sort(list(local_failure_set.keys()))\n",
    "            t_index = np.where(unique_failure_times == t)[0][0]\n",
    "            if t_index < len(unique_failure_times) - 1: # not last\n",
    "                local_risk_set[t] += \\\n",
    "                    local_risk_set[unique_failure_times[t_index+1]]\n",
    "            else:\n",
    "                local_risk_set[t] += round(x*sum(res_t>=t)/len(res_t))\n",
    "        risk_times = [w for w in local_risk_set.keys() if w <= t]\n",
    "        for at_risk_time in risk_times:\n",
    "            local_risk_set[at_risk_time] += 1.0\n",
    "        \n",
    "\n",
    "    # Compute KM estimator\n",
    "    unique_failure_times = np.sort(list(local_failure_set.keys()))\n",
    "    discrete_hazard = np.array(\n",
    "        [local_failure_set[j]/local_risk_set[j] for j in unique_failure_times]\n",
    "    )\n",
    "    km = np.cumprod(1-discrete_hazard)\n",
    "    km_dict = dict(zip(unique_failure_times, km))\n",
    "    return {\"failure_set\": local_failure_set, \"risk_set\": local_risk_set,\n",
    "            \"km\": km_dict}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate right-censored data\n",
    "    nr.seed(1) # for replicability\n",
    "    n = 100\n",
    "    true_failure_times = nr.exponential(size=n)*100\n",
    "    censoring_times = nr.exponential(size=n)*100\n",
    "\n",
    "    # Note that you could do this in one step rather than two\n",
    "    observed_times = np.array(\n",
    "        [min(t, c) for t, c in zip(true_failure_times, censoring_times)]\n",
    "    )\n",
    "    censoring_indicator = np.array(\n",
    "        [t <= c for t, c in zip(true_failure_times, censoring_times)]\n",
    "    )\n",
    "    km_est = streaming_km(observed_times, censoring_indicator)\n",
    "    plt.plot(km_est[\"km\"].keys(), km_est[\"km\"].values())\n",
    "    ecdf = ECDF(true_failure_times)(\n",
    "        np.linspace(min(observed_times), max(observed_times))\n",
    "    )\n",
    "    plt.plot(np.linspace(min(observed_times), max(observed_times)), 1-ecdf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f981f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Question 2 \n",
    "\n",
    "#### $\\hat{\\beta}_n^{\\lambda}$ as the LASSO Estimator\n",
    "\n",
    "Consider observed iid data ${(\\mathbf{X}_i, Y_i)}_{i=1}^n$ and with $\\lambda > 0$, define $(\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n) = \\text{arg min}_{\\mathbf{(u,v)}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + \\lambda||\\mathbf{v}||^2/2$ with $\\hat{\\beta}_n^{\\lambda} = \\hat{\\mathbf{u}}_n \\circ \\hat{\\mathbf{v}}_n$. Now, we note that since $\\circ$ is the element-wise multiplication of vectors, we can also consider the symbol $/$ to be the element-wise division of vectors, such that $\\mathbf{u} = \\beta / \\mathbf{v}$.\n",
    "\n",
    "Then:\n",
    "\\begin{align*}\n",
    "(\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n) &= \\text{arg min}_{\\mathbf{(u,v)}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + \\lambda||\\mathbf{v}||^2/2 \\\\\n",
    "&= \\text{min}_{\\mathbf{\\beta, v}} \\mathbb{P}_n \\{Y - ((\\beta / \\mathbf{v}) \\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2 \\\\\n",
    "&= \\text{min}_{\\mathbf{\\beta, v}} \\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2\n",
    "\\end{align*}\n",
    "\n",
    "Now, rather than minimize both $\\beta$ and $\\mathbf{v}$ simultaneously, we can first consider minimizing over $\\mathbf{v}$ and then over $\\beta$:\n",
    "\\begin{align*}\n",
    "&= \\text{min}_{\\beta} (\\text{min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2) \\\\\n",
    "&= \\text{min}_{\\beta} (\\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + (\\text{min}_{\\mathbf{v}}(\\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2)) \\\\\n",
    "&= \\text{min}_{\\beta} (\\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda(\\text{min}_{\\mathbf{v}}(||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2)/2) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "To find the minimum over $\\mathbf{v}$ for $||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2$, we take the derivative with respect to $\\mathbf{v}$ and set it to 0.\n",
    "\\begin{align*}\n",
    "0 &= \\frac{\\delta}{\\delta \\mathbf{v}} ||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2 \\\\\n",
    "0 &= - \\frac{\\beta}{\\mathbf{v}} \\frac{\\beta}{\\mathbf{v}^2} + 2 v = - \\frac{2\\beta^2}{\\mathbf{v}^3} + 2v = 2v - \\frac{2\\beta^2}{v^3} \\\\\n",
    "0 &= 2\\mathbf{v}^4 - 2\\beta^2 \\mathbf{v}^4 = \\beta^2 \\\\\n",
    "\\mathbf{v}^4 &= \\beta^2 \\\\\n",
    "\\mathbf{v}^2 &= |\\beta|\n",
    "\\end{align*}\n",
    "\n",
    "Thus, $||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2$ is minimized at $\\mathbf{v}^2 = |\\beta|$. Now we find the actual value of $\\text{min}_{\\mathbf{v}}(||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2)/2$ by using this minimum $\\mathbf{v}$ which we will refer to as $\\tilde{\\mathbf{v}}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{min}_{\\mathbf{v}}(||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2) &= ||\\beta / \\tilde{\\mathbf{v}}||^2 + ||\\tilde{\\mathbf{v}}||^2 \\\\\n",
    "&= \\sum_{j=1}^p (\\beta_j^2 / \\tilde{\\mathbf{v}}_j^2 + \\tilde{\\mathbf{v}}_j^2) \\\\\n",
    "&= \\sum_{j=1}^p (\\beta_j^2 / |\\beta_j| + |\\beta_j|) = \\sum_{j=1}^p (|\\beta_j| + |\\beta_j|) \\\\\n",
    "&= \\sum_{j=1}^p 2|\\beta_j| = 2\\sum_{j=1}^p |\\beta_j|\\\\\n",
    "&= 2||\\beta||_1\n",
    "\\end{align*}\n",
    "\n",
    "Plugging this result into the earlier equation, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{min}_{\\beta} (\\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda(\\text{min}_{\\mathbf{v}}(||\\beta / \\mathbf{v}||^2 + ||\\mathbf{v}||^2)/2) &= \\text{min}_{\\beta} (\\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda(2||\\beta||_1)/2) \\\\\n",
    "&= \\text{min}_{\\beta} (\\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda||\\beta||_1\n",
    "\\end{align*}\n",
    "\n",
    "This matches the forumula for lasso with $\\lambda$ as the tuning parameter, thus $\\hat{\\beta}_n^{\\lambda} = \\hat{\\mathbf{u}}_n \\circ \\hat{\\mathbf{v}}_n$ is the lasso estimator with $\\lambda$ as the tuning parameter.\n",
    "\n",
    "#### Alternating Ridge Regression\n",
    "\n",
    "Once we recognize that $\\hat{\\beta}_n^{\\lambda}$ is the LASSO estimator with tuning parameter $\\lambda$, we can now consider implementing a lasso estimator from this representation. First, note that since multiplication is commutative under the real numbers, $\\mathbf{u} \\circ \\mathbf{v} = \\begin{pmatrix} u_1 * v_1 & ... & u_p * v_p \\end{pmatrix} = \\begin{pmatrix} v_1 * u_1 & ... & v_p * u_p \\end{pmatrix} = \\mathbf{v} \\circ \\mathbf{u}$, and therefore $(\\mathbf{u}\\circ\\mathbf{v})\\mathbf{X} = (\\mathbf{v}\\circ\\mathbf{u})\\mathbf{X}$. Now consider the initial definition of the minimization function given above. Consider an initial $\\mathbf{v}_0$. Now, we have: \n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{u}_1} &= \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v}_0)^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + \\lambda||\\mathbf{v}_0||^2/2 \\\\\n",
    "&= \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v}_0)^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + const.\n",
    "\\end{align*}\n",
    "\n",
    "Now, we can consider the matrix $\\mathbf{X}^*$, defined by taking the elements $v_i$ of $\\mathbf{v}_0$ and multiplying $v_i$ by the $i^{th}$ row of $\\mathbf{X}$. In this construction, $\\mathbf{u}\\mathbf{X}^* = (\\mathbf{u}\\circ\\mathbf{v}_0)\\mathbf{X}$. Then we have $\\hat{\\mathbf{u}_1} = \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - \\mathbf{u}^T\\mathbf{X}^*\\}^2 + \\lambda||\\mathbf{u}||^2/2 + const.$. Now, since we are minimizing with respect to $\\mathbf{u}$, and the constant $\\lambda||\\mathbf{v}_0||^2/2$ does not contain $\\mathbf{u}$, we can ignore it with respect to the minimization (since for every $\\mathbf{u}$, we would simply be adding on the same extra term). Then, we have $\\hat{\\mathbf{u}_1} = \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - \\mathbf{u}^T\\mathbf{X}^*\\}^2 + \\lambda||\\mathbf{u}||^2/2$, which is simply the equation that minimizes to Ridge Regression (with $\\lambda/2$ instead of $\\lambda$, but we could simply let $\\lambda^* = \\lambda/2$ to avoid this). As such, we have used this representation to minimize $\\mathbf{u}$ given a fixed $\\mathbf{v}$ by Ridge Regression.\n",
    "\n",
    "Now, with $\\hat{\\mathbf{u}_1}$, consider the matrix $\\mathbf{X}^{\\circ}$, defined by taking the elements $u_i$ of $\\hat{\\mathbf{u}_1}$ and multiplying $u_i$ by the $i^{th}$ row of $\\mathbf{X}$. With this construction, $\\mathbf{v}\\mathbf{X}^{\\circ} = (\\mathbf{v} \\circ \\hat{\\mathbf{u}_1})\\mathbf{X}$. Then, we can consider the minimization of $\\mathbf{v}$:\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{v}_1} &= \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - (\\mathbf{v}\\circ\\hat{\\mathbf{u}_1})^T\\mathbf{X}\\}^2 + \\lambda||\\hat{\\mathbf{u}_1}||^2/2 + \\lambda||\\mathbf{v}||^2/2 \\\\\n",
    "&= \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||^2/2 + \\lambda||\\hat{\\mathbf{u}_1}||^2/2\n",
    "\\end{align*}\n",
    "\n",
    "Now we have $\\hat{\\mathbf{v}_1} = \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||^2/2 + const.$, and as with the minimization of $\\mathbf{u}$ previously, since we are minimizating with respect to $\\mathbf{v}$, $\\lambda||\\hat{\\mathbf{u}_1}||^2/2$ is a constant, and so we can ignore it for the sake of minimization, and thus we instead have $\\hat{\\mathbf{v}_1} = \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||^2/2$, which is the equation that minimizes Ridge Regression. As such, we have also minimized $\\mathbf{v}$ given $\\hat{\\mathbf{u}_1}$ by Ridge Regression.\n",
    "\n",
    "We can now repeat the above process to find $\\hat{\\mathbf{u}_i}, \\hat{\\mathbf{v}_i}$ through a series of alternating Ridge Regressions, first minimizing $\\mathbf{u}$ using $\\hat{\\mathbf{v}_{i-1}}$, and then minimizing $\\mathbf{v}$ using $\\hat{\\mathbf{u}_i}$ It is then reasonable to expect that at each iteration $i$, we would then check if $(\\hat{\\mathbf{u}_i}, \\hat{\\mathbf{v}_i})$ actually minimizes the original equation, and we would stop when the original equation was minimized (without a closed form, we could do multiple starts of the with different $\\mathbf{v}_0$ and accept the best minimized function to give us $(\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n)$, which can then be used to calculate $\\hat{\\beta}_n^{\\lambda}$. As such, we have shown that, using this representation, we can implement lasso as a sequence of alternating Ridge Regressions using stochastic approximation methods.\n",
    "\n",
    "It is important to note that the above method does not include any dampening. We could choose to include dampening, within the above framework, by initializing $\\mathbf{u}_0, \\mathbf{v}_0$ and then, for any iteration $i$, calculating $\\mathbf{u}_i = \\mathbf{u}_{i-1} + \\alpha_i \\mathbf{u}^*$ where $\\mathbf{u}^*$ is the solution of the minimization with respect to $\\mathbf{u}$ using $\\mathbf{v}_{i-1}$, and then calculating $\\mathbf{v}_i = \\mathbf{v}_{i-1} + \\alpha_i \\mathbf{v}^*$ where $\\mathbf{v}^*$ is the solution of the minimization with respect to $\\mathbf{v}$ using $\\mathbf{u}_i$, with decreasing step sizes $\\alpha_i$ that satisfy the conditions $\\sum_{i \\ge 1} \\alpha_i = \\infty$ and $\\sum_{i \\ge 1} \\alpha_i^2 < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes {-}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1175b770",
   "metadata": {},
   "source": [
    "1. Peter D. Hoff,\n",
    "Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization,\n",
    "Computational Statistics & Data Analysis,\n",
    "Volume 115,\n",
    "2017,\n",
    "Pages 186-198,\n",
    "ISSN 0167-9473,\n",
    "https://doi.org/10.1016/j.csda.2017.06.007.\n",
    "(https://www.sciencedirect.com/science/article/pii/S0167947317301469)\n",
    "Abstract: Using a multiplicative reparametrization, it is shown that a subclass of Lq penalties with q less than or equal to one can be expressed as sums of L2 penalties. It follows that the lasso and other norm-penalized regression estimates may be obtained using a very simple and intuitive alternating ridge regression algorithm. As compared to a similarly intuitive EM algorithm for Lq optimization, the proposed algorithm avoids some numerical instability issues and is also competitive in terms of speed. Furthermore, the proposed algorithm can be extended to accommodate sparse high-dimensional scenarios, generalized linear models, and can be used to create structured sparsity via penalties derived from covariance models for the parameters. Such model-based penalties may be useful for sparse estimation of spatially or temporally structured parameters.\n",
    "Keywords: Cyclic coordinate descent; Generalized linear model; Linear regression; Optimization; Ridge regression; Sparsity; Spatial autocorrelation\n"
   ]
  }
 ],
 "metadata": {
  "date": "February 20, 2023",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "title": "STA 561 Homework 4"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
