{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22882a3-2af9-44ed-829d-d254864ed184",
   "metadata": {},
   "source": [
    "Authors  \n",
    " -  Alonso Guerrero Castaneda (UID: 1194613)  \n",
    " -  Eli Gnesin (UID: 1172961)  \n",
    " -  Tommy Misikoff (UID: 1166813)  \n",
    " -  Sanskriti Purohit (UID: 1179957)  \n",
    " -  Will Tirone (UID: 1130904)  \n",
    "\n",
    "TA: Rick Presman  \n",
    "\n",
    "The homework is divided into two parts within the *.ipynb* file.\n",
    "\n",
    "    1. Function Definition.\n",
    "    2. Function Implementation.\n",
    "    \n",
    " **Function Definition**\n",
    " \n",
    "The function *tune_bb* is defined to take inputs to automate the tuning of blackbox regression methods, more information on the inputs is included within the function definiton. To ensure stability of the function, error cases are defined. Further, the training data is standardized to make it internally consistent. Following which regularization-specific tuning is mapped out where-in the k-fold (with a user-defined value for k) cross validation is performed to achieve optimum values for the parameters which are used to generate more data. Once, the optimum parameters are achieved and more data is generated, the existing training data is transformed and the tuned model is fitted and returned.\n",
    " \n",
    " **Function Implementation**\n",
    " \n",
    " The function is implemented on the *iris* dataset to demonstrate the function's use on two regression methods, Ridge Regression and Linear Regression with MSE and MAD/MAE as their criterions respectively. It can be observed that the coeffecients and the criterions obtained by all 3 methods when applied the two models achieve approximately the same result, with minor deviations.\n",
    " \n",
    " \n",
    "Similarly, the function can be implemented on different datasets with other blackbox models to optimize a specified criterion.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30174497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add6bf93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tune_bb(algo, X, y, regularization=\"Dropout\", M=10, c=None, K=5, criterion=\"MSE\"):\n",
    "\n",
    "    \"\"\"function to automatically tune blackbox regression model\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    algo : callable\n",
    "        A learning algorithm that takes as input a matrix X in R nxp\n",
    "        and a vector of responses Y in Rn and returns a function that\n",
    "        maps inputs to outputs. Must have methods like .fit() and .predict()\n",
    "    X : array-like of shape (n,p)\n",
    "        training data X in R nxp\n",
    "    y : array-like of shape (n,)\n",
    "        training labels, Y, in Rn\n",
    "    regularization : str, default=\"Dropout\"\n",
    "        regularization method, can be any of \"Dropout\",\n",
    "        \"NoiseAddition\", or \"Robust\"\n",
    "    M : int, default=\n",
    "        A positive integer indicating the number of Monte Carlo\n",
    "        replicates to be used if the method specified is Dropout or \n",
    "        NoiseAddition\n",
    "    c : default=None\n",
    "        A vector of column bounds to be used if method specified is \"Robust\"\n",
    "    K : int, default=5\n",
    "        A positive integer indicating the number of CV-folds to be used to \n",
    "        tune the amount of regularization, e.g., K = 5 indicates five-fold CV\n",
    "    criterion : str, default=\"MSE\"\n",
    "        A criterion to be used to evaluate the method that belongs to the set \n",
    "        {MSE, MAD} where MSE encodes mean square error and MAD encodes mean\n",
    "        absolute deviation.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    tuned_model : callable \n",
    "        A tuned predictive model that optimizes the specific criterion using \n",
    "        the specified method\n",
    "    \"\"\"\n",
    "    \n",
    "    # statements here to ensure model has the methods we need to tune it\n",
    "    assert hasattr(algo, \"fit\"), \"model object must have .fit() method\"\n",
    "    assert hasattr(algo, \"predict\"), \"model object must have .predict() method\"\n",
    "\n",
    "    if criterion == \"MSE\":\n",
    "        criterion = \"neg_mean_squared_error\"\n",
    "    elif criterion == \"MAE\":\n",
    "        criterion = \"neg_mean_absolute_error\"\n",
    "    else:\n",
    "        raise ValueError(\"Please input either MAE or MSE for criterion.\")\n",
    "    \n",
    "    # Standardize X (useful for all methods with regularization)\n",
    "    scaler = StandardScaler()  \n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    if regularization == \"Dropout\":\n",
    "        \n",
    "        # parameter to tune\n",
    "        phi_range = np.linspace(0,1,101)\n",
    "        min_metric = None\n",
    "        best_phi = None\n",
    "        for phi in phi_range:  \n",
    "            metric = []\n",
    "            # CV over K Folds\n",
    "            for m in range(M):\n",
    "                dropout_matrix = np.random.binomial(1,phi,size=X.shape) * X\n",
    "                kf = KFold(n_splits=K)\n",
    "                for train_index, test_index in kf.split(dropout_matrix):\n",
    "                        X_train, y_train = dropout_matrix[train_index], y[train_index]\n",
    "                        X_test, y_test = dropout_matrix[test_index], y[test_index]\n",
    "                        model = algo\n",
    "                        model.fit(X_train, y_train)\n",
    "                        if criterion == \"neg_mean_squared_error\":\n",
    "                            metric.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "                        else:\n",
    "                            metric.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "                            \n",
    "            new_metric = np.mean(metric)\n",
    "            if (min_metric == None or new_metric < min_metric):\n",
    "                best_phi = phi\n",
    "                min_metric = new_metric\n",
    "        \n",
    "        # make a dropout matrix with the best choice of phi \n",
    "        dropout_matrix = np.random.binomial(1,best_phi,size=X.shape) * X\n",
    "        model = algo\n",
    "        tuned_model = model.fit(dropout_matrix, y)\n",
    "\n",
    "    elif regularization == \"NoiseAddition\":\n",
    "        # possible levels of noise\n",
    "        lambda_levels = np.linspace(0, 5, 101)\n",
    "        min_metric = None\n",
    "        best_lambda = None\n",
    "        for lam in lambda_levels: \n",
    "            #CV\n",
    "            metric = []\n",
    "            for m in range(M):\n",
    "                # generate noise matrix with lambda (variance, not std)\n",
    "                Z = np.random.normal(0, lam**2, size=X.shape)\n",
    "                kf = KFold(n_splits=K)\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    X_train, y_train = X[train_index], y[train_index]\n",
    "                    X_test, y_test = X[test_index], y[test_index]\n",
    "                    X_train_disturbed = X_train + Z[train_index]\n",
    "                    model = algo\n",
    "                    model.fit(X_train_disturbed, y_train)\n",
    "                    if criterion == \"neg_mean_squared_error\":\n",
    "                        metric.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "                    else:\n",
    "                        metric.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "            new_metric = np.mean(metric)\n",
    "            if (min_metric == None or new_metric < min_metric):\n",
    "                best_lambda = lam\n",
    "                min_metric = new_metric\n",
    "        \n",
    "        Z = np.random.normal(0, best_lambda**2, size=X.shape)\n",
    "        new_X = X + Z\n",
    "        model = algo\n",
    "        tuned_model = model.fit(new_X, y)\n",
    "        \n",
    "    elif regularization == \"Robust\":\n",
    "        tol = False #Are we in our tolerance range\n",
    "        toler = 5e-4 \n",
    "        wts = [x/2 for x in c] #Initial weights are going to be c/2\n",
    "        oerror = np.inf \n",
    "        merror = np.inf \n",
    "        itera = 0 \n",
    "        while not tol:\n",
    "            #Create a bunch of matrices and choose the best by a score\n",
    "            maxmatrix = None\n",
    "            maxnorm = -np.inf\n",
    "            for i in range(1000):\n",
    "                matrix = np.random.rand(X.shape[0], X.shape[1])\n",
    "                for m in range(matrix.shape[1]):\n",
    "                    matrix[:,m] = (wts[m] / np.linalg.norm(matrix[:,m], 2)) * matrix[:,m]\n",
    "                fnorm = np.linalg.norm(matrix, 2) #The criteria I'm using here is the two-norm\n",
    "                if fnorm > maxnorm:\n",
    "                    maxnorm = fnorm\n",
    "                    maxmatrix = matrix\n",
    "            new_X = X + maxmatrix #We add the permuted matrix to our design matrix\n",
    "            \n",
    "            #kfold cross validation\n",
    "            errors = np.abs(cross_val_score(algo, new_X, y, cv=K, scoring=criterion))\n",
    "            merror = np.mean(errors)\n",
    "            if abs(oerror - merror) > toler:\n",
    "                oerror = merror\n",
    "                #Set our new weights for the next iteration\n",
    "                wts = np.minimum(wts + (np.random.normal(size = len(wts)) * math.exp(-itera/2)), c)\n",
    "                wts = np.maximum(0.1, wts) #weights can't be negative\n",
    "                itera += 1\n",
    "            else:\n",
    "                tol = True\n",
    "        \n",
    "        #Once we have our best c bounds, let's use them exactly to construct the best model\n",
    "        maxmatrix = None\n",
    "        maxnorm = -np.inf\n",
    "        for i in range(10000):\n",
    "            matrix = np.random.rand(X.shape[0], X.shape[1])\n",
    "            for m in range(matrix.shape[1]):\n",
    "                matrix[:,m] = (wts[m] / np.linalg.norm(matrix[:,m], 2)) * matrix[:,m]\n",
    "            fnorm = np.linalg.norm(matrix, 2) #The criteria I'm using here is the two-norm\n",
    "            if fnorm > maxnorm:\n",
    "                maxnorm = fnorm\n",
    "                maxmatrix = matrix\n",
    "        new_X = X + maxmatrix #We add the permuted matrix to our design matrix\n",
    "        \n",
    "        model = algo\n",
    "        tuned_model = model.fit(new_X, y)\n",
    "    else: \n",
    "        raise ValueError('Please input one of of \"Dropout\", \"NoiseAddition\", or \"Robust\"')\n",
    "\n",
    "    return tuned_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319e632",
   "metadata": {},
   "source": [
    "### Function Demonstration {-}\n",
    "\n",
    "Below we fit our function on the iris data and show that the three regularization methods give identical answers for the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed959b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtune_bb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mregularization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Dropout'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MSE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "function to automatically tune blackbox regression model\n",
       "\n",
       "Parameters:\n",
       "-----------\n",
       "\n",
       "algo : callable\n",
       "    A learning algorithm that takes as input a matrix X in R nxp\n",
       "    and a vector of responses Y in Rn and returns a function that\n",
       "    maps inputs to outputs. Must have methods like .fit() and .predict()\n",
       "X : array-like of shape (n,p)\n",
       "    training data X in R nxp\n",
       "y : array-like of shape (n,)\n",
       "    training labels, Y, in Rn\n",
       "regularization : str, default=\"Dropout\"\n",
       "    regularization method, can be any of \"Dropout\",\n",
       "    \"NoiseAddition\", or \"Robust\"\n",
       "M : int, default=\n",
       "    A positive integer indicating the number of Monte Carlo\n",
       "    replicates to be used if the method specified is Dropout or \n",
       "    NoiseAddition\n",
       "c : default=None\n",
       "    A vector of column bounds to be used if method specified is \"Robust\"\n",
       "K : int, default=5\n",
       "    A positive integer indicating the number of CV-folds to be used to \n",
       "    tune the amount of regularization, e.g., K = 5 indicates five-fold CV\n",
       "criterion : str, default=\"MSE\"\n",
       "    A criterion to be used to evaluate the method that belongs to the set \n",
       "    {MSE, MAD} where MSE encodes mean square error and MAD encodes mean\n",
       "    absolute deviation.\n",
       "\n",
       "Returns:\n",
       "-----------\n",
       "tuned_model : callable \n",
       "    A tuned predictive model that optimizes the specific criterion using \n",
       "    the specified method\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\elign\\appdata\\local\\temp\\ipykernel_32024\\1101274168.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use this to view doc string\n",
    "?tune_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b601f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting full iris data set to train our model\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f630f802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust_Ridge Regression Coefficients :  [-0.07314516 -0.02833511  0.41251747  0.42918069]\n",
      "Dropout_Ridge Regression Coefficients :  [-0.07346142 -0.02451997  0.37922144  0.46389549]\n",
      "Noise_Ridge Regression Coefficients :  [-0.07381698 -0.02439013  0.38086222  0.46235169]\n",
      "\n",
      "Robust_Ridge MSE :  2.214925565757384\n",
      "Dropout_Ridge MSE :  2.263664317895736\n",
      "Noise_Ridge MSE :  2.271619768722753\n"
     ]
    }
   ],
   "source": [
    "# An example of all 3 regularization types with Ridge() and MSE\n",
    "Robust_Ridge = tune_bb(Ridge(),\n",
    "                       X,\n",
    "                       y,\n",
    "                       regularization=\"Robust\",\n",
    "                       c = [4,5,4,3],\n",
    "                       criterion=\"MSE\")\n",
    "\n",
    "Dropout_Ridge = tune_bb(Ridge(),\n",
    "                        X,\n",
    "                        y,\n",
    "                        regularization=\"Dropout\",\n",
    "                        criterion=\"MSE\")\n",
    "\n",
    "Noise_Ridge = tune_bb(Ridge(),\n",
    "                      X,\n",
    "                      y,\n",
    "                      regularization=\"NoiseAddition\",\n",
    "                      criterion=\"MSE\")\n",
    "\n",
    "print(\"Robust_Ridge Regression Coefficients : \", Robust_Ridge.coef_)\n",
    "print(\"Dropout_Ridge Regression Coefficients : \", Dropout_Ridge.coef_)\n",
    "print(\"Noise_Ridge Regression Coefficients : \", Noise_Ridge.coef_)\n",
    "\n",
    "rr = Robust_Ridge.predict(X)\n",
    "dr = Dropout_Ridge.predict(X)\n",
    "nr = Noise_Ridge.predict(X)\n",
    "\n",
    "print()\n",
    "print(\"Robust_Ridge MSE : \", mean_squared_error(y, rr))\n",
    "print(\"Dropout_Ridge MSE : \", mean_squared_error(y, dr))\n",
    "print(\"Noise_Ridge MSE : \", mean_squared_error(y, nr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393ead4f-6888-421d-a0ca-d0fbf93b734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust_Linear Regression Coefficients :  [-0.0896758  -0.01689165  0.42533297  0.43754402]\n",
      "Dropout_Linear Regression Coefficients :  [-0.09235605 -0.01741097  0.40227899  0.46284429]\n",
      "Noise_Linear Regression Coefficients :  [-0.09127121 -0.01827394  0.40174911  0.46173039]\n",
      "\n",
      "Robust_Linear MAE :  1.50591354209955\n",
      "Dropout_Linear MAE :  1.4739707410920417\n",
      "Noise_Linear MAE :  1.474168983177445\n"
     ]
    }
   ],
   "source": [
    "# An example of all 3 regularization types with LinearRegression() and MAE\n",
    "Robust_Linear = tune_bb(LinearRegression(),\n",
    "                       X,\n",
    "                       y,\n",
    "                       regularization=\"Robust\",\n",
    "                       c = [4,5,4,3],\n",
    "                       criterion=\"MAE\")\n",
    "\n",
    "Dropout_Linear = tune_bb(LinearRegression(),\n",
    "                        X,\n",
    "                        y,\n",
    "                        regularization=\"Dropout\",\n",
    "                        criterion=\"MAE\")\n",
    "\n",
    "Noise_Linear = tune_bb(LinearRegression(),\n",
    "                      X,\n",
    "                      y,\n",
    "                      regularization=\"NoiseAddition\",\n",
    "                      criterion=\"MAE\")\n",
    "\n",
    "print(\"Robust_Linear Regression Coefficients : \", Robust_Linear.coef_)\n",
    "print(\"Dropout_Linear Regression Coefficients : \", Dropout_Linear.coef_)\n",
    "print(\"Noise_Linear Regression Coefficients : \", Noise_Linear.coef_)\n",
    "\n",
    "rl = Robust_Linear.predict(X)\n",
    "dl = Dropout_Linear.predict(X)\n",
    "nl = Noise_Linear.predict(X)\n",
    "\n",
    "print()\n",
    "print(\"Robust_Linear MAE : \", mean_absolute_error(y, rl))\n",
    "print(\"Dropout_Linear MAE : \", mean_absolute_error(y, dl))\n",
    "print(\"Noise_Linear MAE : \", mean_absolute_error(y, nl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes: {-}\n",
    "\n",
    "1. https://www.statology.org/k-fold-cross-validation-in-python/"
   ]
  }
 ],
 "metadata": {
  "date": "February 20, 2023",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "title": "STA 561 Homework 3"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
