{"cells":[{"cell_type":"markdown","metadata":{"id":"T-nPSPzwQNG-"},"source":["### STA 561 Homework 2\n","#### *Due Feb 3, 2023*\n","Authors\n"," -  Alonso Guerrero Castaneda (UID: 1194613)\n"," -  Eli Gnesin (UID: 1172961)\n"," -  Tommy Misikoff (UID: 1166813)\n"," -  Sanskriti Purohit (UID: 1179957)\n"," -  Will Tirone (UID: 1130904)\n","\n","TA: Rick Presman"]},{"cell_type":"markdown","metadata":{"id":"45BbX6bfQrkT"},"source":["### Question 1\n","\n","#### a)   \n","\n","$$\n","\\begin{aligned}\n","E[Z \\textbf{1}_{|Z| \\le γ}] &= \\tau E[\\frac{Z-\\omega}{\\tau}\\textbf{1}_{-\\gamma \\le Z \\le \\gamma}] + \\omega E[\\textbf{1}_{-\\gamma \\le Z \\le \\gamma}] \\\\\n","&= \\tau E[\\frac{Z - \\omega}{\\tau}\\textbf{1}_{\\frac{-\\gamma - \\omega}{\\tau} \\le \\frac{Z - \\omega}{\\tau} \\le \\frac{\\gamma - \\omega}{\\tau}}] + \\omega P(-\\gamma \\le Z \\le \\gamma) \\\\\n","&= \\tau E[\\frac{Z - \\omega}{\\tau}\\textbf{1}_{\\frac{-\\gamma - \\omega}{\\tau} \\le \\frac{Z - \\omega}{\\tau} \\le \\frac{\\gamma - \\omega}{\\tau}}] + \\omega P(\\frac{-\\gamma - \\omega}{\\tau} \\le \\frac{Z - \\omega}{\\tau} \\le \\frac{\\gamma - \\omega}{\\tau})\n","\\end{aligned}\n","$$\n","Now let $\\tilde{Z} = \\frac{Z - \\omega}{\\tau} \\sim Normal(0,1)$.\n","$$\n","\\begin{aligned}\n","&= \\tau E[\\tilde{Z}\\textbf{1}_{\\frac{-\\gamma - \\omega}{\\tau} \\le \\tilde{Z} \\le \\frac{\\gamma - \\omega}{\\tau}}] + \\omega P(\\frac{-\\gamma - \\omega}{\\tau} \\le \\tilde{Z} \\le \\frac{\\gamma - \\omega}{\\tau}) \\\\\n","&= \\frac{\\tau}{\\sqrt{2\\pi}} \\int_{\\frac{-\\gamma - \\omega}{\\tau}}^{\\frac{\\gamma - \\omega}{\\tau}} x e^{-\\frac{x^2}{2}} dx + \\omega P(\\frac{-\\gamma - \\omega}{\\tau} \\le \\tilde{Z} \\le \\frac{\\gamma - \\omega}{\\tau}) \\\\\n","&= \\frac{\\tau}{\\sqrt{2\\pi}} \\int_{lb}^{ub} -e^{u} dx + \\omega P(\\frac{-\\gamma - \\omega}{\\tau} \\le \\tilde{Z} \\le \\frac{\\gamma - \\omega}{\\tau}) \\quad \\quad u = -\\frac{x^2}{2}, du = -x dx \\\\\n","&= \\frac{\\tau}{\\sqrt{2\\pi}}  -e^{-\\frac{x^2}{2}}\\rvert_{\\frac{-\\gamma - \\omega}{\\tau}}^{\\frac{\\gamma - \\omega}{\\tau}} + \\omega P(\\frac{-\\gamma - \\omega}{\\tau} \\le \\tilde{Z} \\le \\frac{\\gamma - \\omega}{\\tau}) \\\\\n","&= \\frac{\\tau}{\\sqrt{2\\pi}}  [e^{((-\\omega - \\gamma)^2/(2 \\tau^2))} - e^{(-\\omega + \\gamma)^2/(2 \\tau^2)}] + \\omega P(\\frac{-\\gamma - \\omega}{\\tau} \\le \\tilde{Z} \\le \\frac{\\gamma - \\omega}{\\tau}) \\\\\n","&= \\frac{\\tau}{\\sqrt{2\\pi}}  [e^{((-\\omega - \\gamma)^2/(2 \\tau^2))} - e^{(-\\omega + \\gamma)^2/(2 \\tau^2)}] +  \\omega (\\Phi(\\frac{\\gamma - \\omega}{\\tau}) - \\Phi(\\frac{-\\gamma - \\omega}{\\tau}))\n","\\end{aligned}\n","$$\n","\n","Thus, we have $E[Z \\textbf{1}_{|Z| \\le γ}] = \\frac{\\tau}{\\sqrt{2\\pi}}  [e^{((-\\omega - \\gamma)^2/(2 \\tau^2))} - e^{(-\\omega + \\gamma)^2/(2 \\tau^2)}] +  \\omega (\\Phi(\\frac{\\gamma - \\omega}{\\tau}) - \\Phi(\\frac{-\\gamma - \\omega}{\\tau}))$ Where $\\Phi(a)$ is the Cumulative distribution function of a standard normal distribution ($\\Phi(a) = \\int_{-\\infty}^a \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} dt$).\n","\n","#### b)\n","Let $\\hat{\\mu}^H_n = \\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha}$\n","\n","$$\n","\\begin{aligned}\n","E(X-\\hat{\\mu}^H_n)^2 &= E(X- \\mu + \\mu -  \\hat{\\mu}^H_n)^2\\\\ \n","&= E\\Big\\{(X-\\mu)^2 + \\underbrace{2(X-\\mu)(\\mu-  \\hat{\\mu}^H_n)}_\\text{=0} + ((\\mu - \\hat{\\mu}^H_n)^2   \\Big\\} \\\\ \n","&= E\\Big\\{(X-\\mu)^2 + (\\mu - \\hat{\\mu}^H_n)^2   \\Big\\} \\\\ \n","&= Var(X) + E(\\mu^2) + E[(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha})^2] - 2\\mu E[(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha})] \\\\\n","&= \\sigma^2 + \\mu^2   + E[(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha})^2] + (E(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha}))^2 - (E(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha}))^2 - 2\\mu E[(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha})] \\\\\n","&= \\sigma^2 + \\mu^2 + Var(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha}) + (E(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha}))^2 - 2\\mu E[(\\bar{X}_n \\textbf{1}_{|\\bar{X}_n| \\ge \\alpha})]\\\\\n","&= \\sigma^2 + \\mu^2 + \\frac{\\sigma^2}{n}\\textbf{1}_{|\\bar{X}_n| \\ge \\alpha} + (E((\\mu - \\bar{X}_n) \\textbf{1}_{|\\bar{X}_n| \\le \\alpha}))^2 - 2\\mu E[((\\mu - \\bar{X}_n) \\textbf{1}_{|\\bar{X}_n| \\le \\alpha})]\\\\\n","&= \\sigma^2 + \\mu^2 + \\frac{\\sigma^2}{n}\\textbf{1}_{|\\bar{X}_n| \\ge \\alpha} + (\\mu E(\\textbf{1}_{|\\bar{X}_n|\\le \\alpha})- E(\\bar{X}_n\\textbf{1}_{|\\bar{X}_n| \\le \\alpha}))^2 - 2\\mu [\\mu E(\\textbf{1}_{|\\bar{X}_n| \\le \\alpha}) - E(\\bar{X}_n\\ \\textbf{1}_{|\\bar{X}_n| \\le \\alpha})]\\\\\n","&= \\sigma^2 + \\mu^2 + \\frac{\\sigma^2}{n}\\textbf{1}_{|\\bar{X}_n| \\ge \\alpha} + (\\mu (\\Phi(\\frac{\\mu - \\alpha}{\\sigma/\\sqrt{n}}) - \\Phi(\\frac{-\\mu - \\alpha}{\\sigma/\\sqrt{n}}))- E(\\bar{X}_n\\textbf{1}_{|\\bar{X}_n| \\le \\alpha}))^2 - 2\\mu [\\mu  (\\Phi(\\frac{\\mu - \\alpha}{\\sigma/\\sqrt{n}}) - \\Phi(\\frac{-\\mu - \\alpha}{\\sigma/\\sqrt{n}})) - E(\\bar{X}_n\\ \\textbf{1}_{|\\bar{X}_n| \\le \\alpha})]\\\\\n","\\end{aligned}\n","$$\n","\n","Since the $X_{is}$ are i.i.d.s., $\\implies \\bar{X}_n \\sim N(\\mu,\\sigma^2/n)$ \n","\n","$$\n","\\begin{aligned}\n","E(X-\\hat{\\mu}^H_n)^2 &= \\sigma^2 + \\mu^2 + \\frac{\\sigma^2}{n}\\textbf{1}_{|\\bar{X}_n| \\ge \\alpha} + (\\mu (\\Phi(\\frac{\\mu - \\alpha}{\\sigma/\\sqrt{n}}) - \\Phi(\\frac{-\\mu - \\alpha}{\\sigma/\\sqrt{n}}))- E(\\bar{X}_n\\textbf{1}_{|\\bar{X}_n| \\le \\alpha}))^2 - 2\\mu [\\mu  (\\Phi(\\frac{\\mu - \\alpha}{\\sigma/\\sqrt{n}}) - \\Phi(\\frac{-\\mu - \\alpha}{\\sigma/\\sqrt{n}})) - E(\\bar{X}_n\\ \\textbf{1}_{|\\bar{X}_n| \\le \\alpha})]\\\\\n","\\end{aligned}\n","$$\n","where we can substitute the parameters of the result obtained in part (a) with those of a $N(\\mu,\\sigma^2/n)$ distribution.\n","\n","\n","We recognize this is incomplete, but we ran out of time to finish it.\n","\n","#### c)\n","Recognizing that we do not have a full answer for part (b), it is impossible to suggest a truly accurate estimator $\\hat{a}_n$ for $a^{opt}$, but our approach would be to take the results from part (b), substitute in $\\mu = \\bar{X_n}$ and $\\sigma^2 = s^2$ as the sample mean and standard deviation from our data, then take the derivative with respect to our hard thresholding estimator $\\hat{\\mu}^H_n$, set the results equal to zero, and solve for $\\hat{\\mu}^H_n$ to be our $\\hat{a}_n$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQCCIlaTMErQ"},"outputs":[],"source":["#Load packages\n","\n","import pandas as pd \n","import plotly.express as px\n","import numpy as np \n","import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LassoLarsIC\n"]},{"cell_type":"markdown","metadata":{"id":"vnfQhch-RST_"},"source":["### Question 2\n","\n","#### a) \n","Assume we have $Y = \\mathbf{X}^T\\beta^*+\\epsilon$ with $\\epsilon \\sim Normal(0,\\sigma^2), \\mathbf{X} \\sim Normal_p(0, \\mathbf{\\Sigma}(\\rho))$, and $\\{\\mathbf{\\Sigma}(\\rho)\\}_{i,j} = \\rho^{\\lvert i - j \\rvert}, \\rho \\in (0,1)$. Now consider a constant $r \\in (0,1)$, and we want to find $\\sigma^2$ such that $R^2 = 1-P(Y-\\mathbf{X}^T\\beta^*)^2/Var(Y) = r$. Then:\n","\n","$$\n","\\begin{aligned}\n","r &= 1-P(Y-\\mathbf{X}^T\\beta^*)^2/Var(Y) \\\\\n","&= 1-P(\\mathbf{X}^T\\beta^* + \\epsilon -\\mathbf{X}^T\\beta^*)^/2Var(\\mathbf{X}^T\\beta^* + \\epsilon) \\\\\n","&= 1-P(\\epsilon)^2/Var(\\mathbf{X}^T\\beta^* + \\epsilon) \\\\\n","&= 1-(Var(\\epsilon) + (P\\epsilon)^2)/Var(\\mathbf{X}^T\\beta^* + \\epsilon) \\\\\n","&= 1-(\\sigma^2 + 0)/Var(\\mathbf{X}^T\\beta^* + \\epsilon) \\\\\n","&= 1-\\sigma^2/Var(\\mathbf{X}^T\\beta^* + \\epsilon) \\\\\n","&= 1-\\sigma^2 /(Var(X^T\\beta^*) + Var(\\epsilon) + 2Cov(X^T\\beta, \\epsilon) \\\\\n","&= 1-\\sigma^2 /(Var(X^T\\beta^*) + \\sigma^2 + 0) \\\\\n","&= 1-\\sigma^2 / (Var(X^T\\beta^*) + \\sigma^2) \\\\\n","1-r &= \\sigma^2 / (Var(X^T\\beta^*) + \\sigma^2) \\\\\n","(1-r)Var(X^T\\beta^*) + (1-r)\\sigma^2 &= \\sigma^2 \\\\\n","(1-r)Var(X^T\\beta^*) &= \\sigma^2 - (1-r)\\sigma^2 \\\\\n","(1-r)Var(X^T\\beta^*) &= r\\sigma^2 \\\\\n","\\frac{(1-r)Var(X^T\\beta^*)}{r} &= \\sigma^2 \\\\\n","\\frac{(1-r)(\\beta^*)^TVar(X^T)\\beta^*}{r} &= \\sigma^2 \\\\\n","\\frac{(1-r)(\\beta^*)^T\\mathbf{\\Sigma}(\\rho)\\beta^*}{r} &= \\sigma^2\n","\\end{aligned}\n","$$\n","\n","Thus, we have $\\sigma^2 = \\frac{(1-r)(\\beta^*)^T\\mathbf{\\Sigma}(\\rho)\\beta^*}{r}$."]},{"cell_type":"markdown","metadata":{"id":"iQhZBcDV0pX0"},"source":["b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4QxlMBz0xJE"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn import linear_model, metrics, model_selection\n","\n","# Parameters\n","\n","n = 100\n","p_vector = [10, 24, 50]\n","ro_vector = [0, 0.25, 0.5]\n","\n","def beta_sparse(p):\n","  j = np.arange(1, p+1)\n","  beta_j = (2/np.sqrt(n))*np.less_equal(j, np.sqrt(p))\n","  return(beta_j)\n","\n","def beta_dense(p):\n","  j = np.arange(1, p+1)\n","  beta_j = 5/(j*np.sqrt(n))\n","  return(beta_j)\n","\n","def cov_matrix(p, ro):\n","  abs_dif = np.empty([p,p])\n","  for i in range(p):\n","    for j in range(p):\n","      abs_dif[i,j] = np.abs(i-j)\n","  cov = ro**abs_dif\n","  return(cov)  \n","\n","def sigma2(cov, beta, r = 0.8):\n","  s2 = (1 - r)*beta.T@cov@beta/r\n","  return(s2)\n","\n","# Data set creation\n","\n","def data_set_creation(sparse, p, ro, stand = True):\n","  \n","  if sparse:\n","    beta = beta_sparse(p)\n","  else: \n","    beta = beta_dense(p)\n","  \n","  X_var = cov_matrix(p, ro)\n","  std_dev = np.sqrt(sigma2(X_var, beta, r=0.8))\n","\n","  X_mean = [0]*p\n","  n = 100\n","  X = np.random.multivariate_normal(X_mean, X_var, n)\n","  eps = np.random.normal(loc=0, scale=np.sqrt(std_dev), size=n)\n","  Y = (X@beta + eps)\n","\n","  # Standardize X\n","  if stand:\n","    X /= np.sqrt(np.sum(X**2, axis=0))\n","  \n","  return X, Y"]},{"cell_type":"code","source":["class CustomRidgeRegression() :  \n","    def __init__(self, l2_penality = 1, fit_intercept=True, criteria='None') :    \n","      self.l2_penality = l2_penality\n","      self.fit_intercept = fit_intercept\n","      self.criteria = criteria\n","\n","    def _fit_model(self, X, Y, l2_penality):\n","      if (self.fit_intercept):\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","        A = np.identity(np.shape(X)[1])\n","        A[0, 0] = 0\n","      else:\n","        A = np.identity(np.shape(X)[1])\n","\n","      # print(l2_penality)\n","      hat_mat = np.linalg.inv(X.transpose()@X + l2_penality * A)@X.transpose()\n","      B = hat_mat@Y\n","      degrees_freedom = np.trace(X@hat_mat)\n","      RSS = np.sum(np.square(X.dot(B) - Y))\n","      return (B, degrees_freedom, RSS)\n","\n","    # Function for model training            \n","    def fit(self, X, Y) :\n","      if (self.criteria == 'aic'):\n","        # print('aic')\n","        penalty_choices = np.linspace(0, 5, 100)\n","        \n","        min_AIC = None\n","        cur_beta = None\n","        for cur_penalty in penalty_choices:\n","          new_beta, new_degrees_of_freedom, new_RSS = self._fit_model(X, Y, cur_penalty)\n","          n = len(Y)\n","          new_AIC = 2 * new_degrees_of_freedom + n * np.log(new_RSS / n)\n","\n","          if (min_AIC == None or new_AIC < min_AIC):\n","            cur_beta = new_beta\n","            min_AIC = new_AIC\n","\n","        # return best beta based on AIC\n","        self.B = cur_beta\n","      elif (self.criteria == 'bic'):\n","        # print('bic')\n","        penalty_choices = np.linspace(0, 5, 100)\n","        \n","        min_BIC = None\n","        cur_beta = None\n","        for cur_penalty in penalty_choices:\n","          new_beta, new_degrees_of_freedom, new_RSS = self._fit_model(X, Y, cur_penalty)\n","          n = len(Y)\n","          new_BIC = np.log(n) * new_degrees_of_freedom + n * np.log(new_RSS / n)\n","\n","          if (min_BIC == None or new_BIC < min_BIC):\n","            cur_beta = new_beta\n","            min_BIC = new_BIC\n","\n","        # return best beta based on BIC\n","        self.B = cur_beta\n","      else:\n","        self.B = self._fit_model(X,Y, self.l2_penality)[0]\n","\n","      return self\n","      \n","    # Hypothetical function  h( x ) \n","    def predict(self, X) :\n","      if (self.fit_intercept):\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","      # print(self.B[1:])  \n","      return X.dot( self.B )"],"metadata":{"id":"Q0SVw1Ow7tNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hcal8Kd0AeAJ"},"outputs":[],"source":["# Fit the model using the AIC criterion\n","def fit_mse(X, Y, estimator = 'Lasso', tuning_method = 'AIC'):\n","  # print(f'Computing')\n","  mse = 0\n","  \n","  if estimator == 'Lasso' and tuning_method == 'AIC':\n","    model = linear_model.LassoLarsIC(criterion=\"aic\")\n","    model.fit(X, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","  elif estimator == 'Lasso' and tuning_method == 'BIC':\n","    model = linear_model.LassoLarsIC(criterion=\"bic\")\n","    model.fit(X, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","  elif estimator == 'Lasso' and tuning_method == 'LOO-CV':\n","    model = linear_model.LassoCV(cv=model_selection.LeaveOneOut())\n","    model.fit(X, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","\n","\n","\n","    \n","  elif estimator == 'Adaptive Lasso' and tuning_method == 'AIC':\n","    reg = linear_model.LinearRegression().fit(X, Y)\n","    alpha = 1/abs(reg.coef_)\n","    X_tilde = X@np.diag(alpha)\n","\n","    model = linear_model.LassoLarsIC(criterion=\"aic\")\n","    model.fit(X_tilde, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  elif estimator == 'Adaptive Lasso' and tuning_method == 'BIC':\n","    reg = linear_model.LinearRegression().fit(X, Y)\n","    alpha = 1/abs(reg.coef_)\n","    X_tilde = X@np.diag(alpha)\n","\n","    model = linear_model.LassoLarsIC(criterion=\"bic\")\n","    model.fit(X_tilde, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  elif estimator == 'Adaptive Lasso' and tuning_method == 'LOO-CV':\n","    reg = linear_model.LinearRegression().fit(X, Y)\n","    alpha = 1/abs(reg.coef_)\n","    X_tilde = X@np.diag(alpha)\n","\n","    model = linear_model.LassoCV(cv=model_selection.LeaveOneOut())\n","    model.fit(X_tilde, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","\n","\n","\n","\n","  elif estimator == 'Ridge' and tuning_method == 'AIC':\n","    model = CustomRidgeRegression(criteria='aic')\n","    model.fit(X, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  elif estimator == 'Ridge' and tuning_method == 'BIC':\n","    model = CustomRidgeRegression(criteria='bic')\n","    model.fit(X, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  elif estimator == 'Ridge' and tuning_method == 'LOO-CV':\n","    model = linear_model.RidgeCV(cv=model_selection.LeaveOneOut())\n","    model.fit(X, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","\n","\n","\n","  elif estimator == 'Adaptive Ridge' and tuning_method == 'AIC':\n","    reg = linear_model.LinearRegression().fit(X, Y)\n","    alpha = 1/abs(reg.coef_)\n","    X_tilde = X@np.diag(alpha)\n","\n","    model = CustomRidgeRegression(criteria='aic')\n","    model.fit(X_tilde, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  elif estimator == 'Adaptive Ridge' and tuning_method == 'BIC':\n","    reg = linear_model.LinearRegression().fit(X, Y)\n","    alpha = 1/abs(reg.coef_)\n","    X_tilde = X@np.diag(alpha)\n","\n","    model = CustomRidgeRegression(criteria='bic')\n","    model.fit(X_tilde, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  elif estimator == 'Adaptive Ridge' and tuning_method == 'LOO-CV':\n","    reg = linear_model.LinearRegression().fit(X, Y)\n","    alpha = 1/abs(reg.coef_)\n","    X_tilde = X@np.diag(alpha)\n","\n","    model = linear_model.RidgeCV(cv=model_selection.LeaveOneOut())\n","    model.fit(X_tilde, Y)\n","    Y_pred = model.predict(X)\n","    mse = metrics.mean_squared_error(Y,Y_pred)\n","\n","  else:\n","    print(f'Estimator / Tuning Method combination not recognized: {estimator} with {tuning_method}')\n","  \n","  return(mse)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7LIeyXLM51A"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","df = pd.DataFrame(columns = ['Estimator', 'Tuning method', 'Sparse', 'p', 'ro','Average MSE'])\n","\n","sparse = [True, False]\n","estimator = [\n","  'Lasso', \n","  'Adaptive Lasso', \n","  'Ridge',\n","  'Adaptive Ridge'\n","]\n","\n","tuning = [\n","  'AIC', \n","  'BIC', \n","  'LOO-CV'\n","]\n","\n","n = 1000\n","short_n = 10\n","\n","for e in estimator:\n","  for t in tuning:\n","    print(f'Computing {e} with {t}')\n","    for s in sparse:\n","      for p in p_vector:\n","        for ro in ro_vector:\n","          mse_vector = []\n","          for i in range(short_n):\n","            X, Y = data_set_creation(s, p, ro)\n","            mse_value = fit_mse(X, Y, estimator = e, tuning_method = t)\n","            mse_vector.append(mse_value)\n","          mse_avg = np.mean(mse_vector)\n","          df = df.append({\n","              'Estimator': e,\n","              'Tuning method': t,\n","              'Sparse': s,\n","              'p': p,\n","              'ro': ro,\n","              'Average MSE': mse_avg\n","          }, ignore_index = True)\n","\n","\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"HsZQBT1Pi98O"},"source":["## c)\n","\n","To avoid having to run the above cell and wait for the results each time, we exported the results as CSV and are loading back in here. If you need the results to grade the homework, please email us and we are happy to provide it."]},{"cell_type":"code","source":["results_df = pd.read_csv('data.csv')\n","results_df = results_df.set_index(\"Estimator\")"],"metadata":{"id":"I8KFoESwtp8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data is organized with the Estimator as the index and then is repeated for every combination of Tuning method, p, and rho."],"metadata":{"id":"AG7AThKixa5d"}},{"cell_type":"code","source":["results_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"C-tBlXttuYKW","executionInfo":{"status":"ok","timestamp":1675554152456,"user_tz":300,"elapsed":175,"user":{"displayName":"Will T","userId":"16947091469695222242"}},"outputId":"78dddce2-b557-4bd8-9c14-31737fd5c365"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          Tuning method  Sparse   p    ro  Average MSE\n","Estimator                                             \n","Lasso               AIC    True  10  0.00     0.050417\n","Lasso               AIC    True  10  0.25     0.062374\n","Lasso               AIC    True  10  0.50     0.070734\n","Lasso               AIC    True  24  0.00     0.053485\n","Lasso               AIC    True  24  0.25     0.061327"],"text/html":["\n","  <div id=\"df-605662d8-2f42-4ab8-ba3c-0d4b8d4de5be\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tuning method</th>\n","      <th>Sparse</th>\n","      <th>p</th>\n","      <th>ro</th>\n","      <th>Average MSE</th>\n","    </tr>\n","    <tr>\n","      <th>Estimator</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>AIC</td>\n","      <td>True</td>\n","      <td>10</td>\n","      <td>0.00</td>\n","      <td>0.050417</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>AIC</td>\n","      <td>True</td>\n","      <td>10</td>\n","      <td>0.25</td>\n","      <td>0.062374</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>AIC</td>\n","      <td>True</td>\n","      <td>10</td>\n","      <td>0.50</td>\n","      <td>0.070734</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>AIC</td>\n","      <td>True</td>\n","      <td>24</td>\n","      <td>0.00</td>\n","      <td>0.053485</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>AIC</td>\n","      <td>True</td>\n","      <td>24</td>\n","      <td>0.25</td>\n","      <td>0.061327</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-605662d8-2f42-4ab8-ba3c-0d4b8d4de5be')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-605662d8-2f42-4ab8-ba3c-0d4b8d4de5be button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-605662d8-2f42-4ab8-ba3c-0d4b8d4de5be');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["Summarizing every method by taking an average of the average MSEs shows that Ridge has a slightly lower method than the others, though only by a moderate amount."],"metadata":{"id":"plGcY2ouxxX3"}},{"cell_type":"code","source":["results_df['Average MSE'].groupby(level='Estimator').mean()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WU13thsywfZq","executionInfo":{"status":"ok","timestamp":1675554197657,"user_tz":300,"elapsed":124,"user":{"displayName":"Will T","userId":"16947091469695222242"}},"outputId":"9dd9f0d5-f989-4bef-f809-d7c8619b444f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Estimator\n","Adaptive Lasso    0.107598\n","Adaptive Ridge    0.095108\n","Lasso             0.092954\n","Ridge             0.086921\n","Name: Average MSE, dtype: float64"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["# performing another grouping to view averages again\n","# reset and set the index first \n","results_df = results_df.reset_index()\n","results_df = results_df.set_index(['Estimator','Tuning method'])"],"metadata":{"id":"TXUKxFrbxEZc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we've done a similar grouping but added the tuning method and summarized. LOO-CV seems to perform well regardless of method (except for adaptive Lasso) wwhile AIC outperforms BIC in every case."],"metadata":{"id":"oKo1-yG50LZP"}},{"cell_type":"code","source":["results_df['Average MSE'].groupby(['Estimator','Tuning method']).mean().sort_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUMIw01gyb0E","executionInfo":{"status":"ok","timestamp":1675554228019,"user_tz":300,"elapsed":1,"user":{"displayName":"Will T","userId":"16947091469695222242"}},"outputId":"8382eb04-b473-4c00-ab58-f56d0177a28e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Estimator       Tuning method\n","Ridge           LOO-CV           0.070747\n","Adaptive Ridge  LOO-CV           0.079808\n","Ridge           AIC              0.081338\n","Lasso           AIC              0.087140\n","                LOO-CV           0.089562\n","Adaptive Ridge  AIC              0.093782\n","Adaptive Lasso  AIC              0.100135\n","Lasso           BIC              0.102161\n","Adaptive Lasso  BIC              0.106194\n","Ridge           BIC              0.108679\n","Adaptive Ridge  BIC              0.111735\n","Adaptive Lasso  LOO-CV           0.116464\n","Name: Average MSE, dtype: float64"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","source":["Visually comparing the different methods, it seems like the choice of `p` has little effect on the MSE. "],"metadata":{"id":"JUhSsgNr06g_"}},{"cell_type":"code","source":["# performing one last grouping to view averages again\n","# reset and set the index first \n","results_df = results_df.reset_index()\n","results_df = results_df.set_index(['Estimator','Tuning method', 'p'])\n","results_df['Average MSE'].groupby(['Estimator','Tuning method','p']).mean().sort_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_KB2cDizxQM","executionInfo":{"status":"ok","timestamp":1675554372952,"user_tz":300,"elapsed":119,"user":{"displayName":"Will T","userId":"16947091469695222242"}},"outputId":"43699efa-0bbf-4552-a4e7-851564717d18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Estimator       Tuning method  p \n","Ridge           LOO-CV         50    0.061314\n","                               24    0.072503\n","Adaptive Ridge  LOO-CV         50    0.073081\n","Ridge           AIC            50    0.078122\n","                LOO-CV         10    0.078424\n","Adaptive Ridge  LOO-CV         24    0.081259\n","Ridge           AIC            24    0.082171\n","                               10    0.083721\n","Lasso           LOO-CV         10    0.083738\n","                AIC            10    0.083980\n","Adaptive Ridge  LOO-CV         10    0.085084\n","Lasso           AIC            24    0.086122\n","                LOO-CV         24    0.086324\n","                BIC            10    0.089600\n","Adaptive Ridge  AIC            50    0.090230\n","                               10    0.090961\n","Adaptive Lasso  AIC            10    0.091107\n","Lasso           AIC            50    0.091318\n","Ridge           BIC            10    0.093390\n","Adaptive Ridge  BIC            10    0.094524\n","Adaptive Lasso  LOO-CV         10    0.094537\n","                BIC            10    0.094905\n","                AIC            24    0.098485\n","Lasso           LOO-CV         50    0.098623\n","Adaptive Ridge  AIC            24    0.100154\n","Lasso           BIC            24    0.101810\n","Adaptive Lasso  BIC            24    0.103273\n","Ridge           BIC            24    0.106233\n","Adaptive Lasso  LOO-CV         24    0.108514\n","                AIC            50    0.110813\n","Adaptive Ridge  BIC            24    0.112193\n","Lasso           BIC            50    0.115075\n","Adaptive Lasso  BIC            50    0.120405\n","Ridge           BIC            50    0.126413\n","Adaptive Ridge  BIC            50    0.128488\n","Adaptive Lasso  LOO-CV         50    0.146339\n","Name: Average MSE, dtype: float64"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["Last, looking at whether or not sparsity affects MSE. From this, it seems sparsity noticeably reduces MSE."],"metadata":{"id":"p74zRHui2Xsf"}},{"cell_type":"code","source":["results_df = results_df.reset_index()\n","results_df = results_df.set_index(['Estimator','Sparse'])\n","results_df['Average MSE'].groupby(['Estimator','Sparse']).mean().sort_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhA_RUUc2amy","executionInfo":{"status":"ok","timestamp":1675554855073,"user_tz":300,"elapsed":165,"user":{"displayName":"Will T","userId":"16947091469695222242"}},"outputId":"5f65a2b6-9dc8-4e50-d2ae-502aae3408d9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Estimator       Sparse\n","Ridge           True      0.070237\n","Adaptive Ridge  True      0.074138\n","Lasso           True      0.076474\n","Adaptive Lasso  True      0.087630\n","Ridge           False     0.103605\n","Lasso           False     0.109434\n","Adaptive Ridge  False     0.116078\n","Adaptive Lasso  False     0.127565\n","Name: Average MSE, dtype: float64"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":[],"metadata":{"id":"FVsjHx7x2hG8"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["T-nPSPzwQNG-"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}